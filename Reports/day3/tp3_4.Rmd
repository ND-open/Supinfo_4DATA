---
title: "TP3 Exercise 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## EXERCISE 4 : Application of data mining: Sentiment analysis or text mining based on reviews left by hotel guests
(from a publication of S. Li and Survey mag)


Numerous studies have shown that the TripAdvisor platform is playing an increasingly important role in a traveler’s decision-making process.

We use a data science, text mining tool to analyze the reviews and comments posted on TripAdvisor of a particular hotel.


We use the Python language to perform this sentiment analysis.

Here is the structure of the sentiment analysis program based on reviews and comments left by customers on TripAdviser. You may have to complete it

### Step 1 : Load libraries


library(dplyr)
library(readr)
library(lubridate)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(stringr)
library(tidyr)
library(scales)
library(broom)
library(purrr)
library(widyr)
library(igraph)
library(ggraph)
library(SnowballC)
library(wordcloud)
library(reshape2)
theme_set(theme_minimal())


### Step 2 : Select the dataset

There have been 13,701 reviews on TripAdvisor for this hotel and the posting dates range from March 21, 2002 to August 2, 2018.
The largest number of reviews on the TripAdvisor site were submitted at the end of 2014. The hotel received more than 70 reviews in one week.

We use the dataset hotel.csv

df <- read_csv("hotel.csv")
df <- df[complete.cases(df), ] 
df$review_date as.date(df$review_date, format =%d-%B-%y>

df %>%
count(Week = round_date(review_date, « week »)) %>%
ggplot(aes(Week, n)) +
geom_line() +
ggtitle(‘The Number of Reviews Per Week’)

### Step 3 : Extract the text of the opinions and comments posted

 

df <- tibble::rowid_to_column(df, "id") df <- df %>%
mutate(review_date = as.POSIXct(review_date, origin = « 1970-01-01 »),month = round_date(review_date, « month »))
review_words <- df %>%
distinct(review_body, .keep_all = TRUE) %>%
unnest_tokens(word, review_body, drop = FALSE) %>%
distinct(ID, word, .keep_all = TRUE) %>%
anti_join(stop_words, by = « word ») %>%
filter(str_detect(word, « [^\\d] »)) %>%
group_by(word) %>%
mutate(word_total = n()) %>%
ungroup()
word_counts <- review_words %>%
count(word, sort = TRUE)
word_counts %>%
head(25) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(fill = « lightblue ») +
scale_y_continuous(labels = comma_format()) +
coord_flip() +
labs(title = « Most common words in review text 2002 to date »,
subtitle = « Among 13,701 reviews; stop words removed »,
y = « # of uses »)

 

Rooting or stemming: This is a process which consists in reducing inflected (or sometimes derived) words to the format of the root, the base or the stem of the word.


  Example "stay" and "stayed", "swimming pool" and "swimming pools".

 

word_counts %>%
head(25) %>%
mutate(word = wordStem(word)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(fill = « lightblue ») +
scale_y_continuous(labels = comma_format()) +
coord_flip() +
labs(title = « Most common words in review text 2002 to date »,
subtitle = « Among 13,701 reviews; stop words removed and stemmed »,
y = « # of uses »)

### Step 4 : Search bigrams

When dealing with text mining, we are led to understand the relationship between words within the same text. It is a question of knowing more precisely: what sequences of words are common? Given a sequence of words, which word is most likely to follow? Which words are correlated?

Many text analyzes are based on these relationships. When we examine pairs of two consecutive words, we speak of bigram.
What are the most used bigrams or word pairs in TripAdvisor reviews for our hotel?

 

review_bigrams <- df %>%
unnest_tokens(bigram, review_body, token = « ngrams », n = 2)
bigrams_separated <- review_bigrams %>%
separate(bigram, c(« word1 », « word2″), sep =  » « )
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep =  » « )
bigrams_united %>%
count(bigram, sort = TRUE)

The answers are “rainbow tower” and “hawaiian village”. We can visualize bigrams in word networks:

 

review_subject <- df %>%
unnest_tokens(word, review_body) %>%
anti_join(stop_words)
my_stopwords <- data_frame(word = c(as.character(1:10)))review_subject <- review_subject %>%
anti_join(my_stopwords)
title_word_pairs <- review_subject %>%
pairwise_count(word, ID, sort = TRUE, upper = FALSE)
set.seed(1234)
title_word_pairs %>%
filter(n >= 1000) %>%
graph_from_data_frame() %>%
ggraph(layout = « fr ») +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = « cyan4 ») +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, « lines »)) +
ggtitle(‘Word network in TripAdvisor reviews’)
theme_void()

The graph produced makes it possible to visualize the common bigrams in TripAdvisor reviews, showing those that have occurred at least 1,000 times and where neither word was - what we call - a stop -word (for example a, an, the). The graph shows strong links between the words at the top ("Hawaiian", "village", "ocean" and "view"). without allowing us to identify a clear grouping structure among the network.

 

### Step 5 : Continue Analysis  with trigrams

Sometimes bigrams are not enough. The correct procedure then is to spot the trigrams. So what are the most common trigrams used in TripAdvisor reviews for the Hotel?

 

review_trigrams <- df %>%
unnest_tokens(trigram, review_body, token = « ngrams », n = 3)

trigrams_separated <- review_trigrams %>%
separate(trigram, c(« word1 », « word2 », « word3″), sep =  » « )

trigrams_filtered <- trigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word3 %in% stop_words$word)

trigram_counts <- trigrams_filtered %>%
count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
unite(trigram, word1, word2, word3, sep =  » « )

trigrams_united %>%
count(trigram, sort = TRUE)

### Step 6 : Grasp trends among the important words expressed in reviews

What words are becoming more and more frequent in the opinions deposited over the years? Conversely, what are the subjects that customers give less importance to? The answers to these two questions help to clarify the crucial points of the hotel, such as service, renovation, problem solving, etc. This ultimately helps to predict the topics that will continue to gain importance with customers - and therefore with hotel management.


Here's the code to find out which words have increased in frequency over time in TripAdvisor reviews:

 

reviews_per_month <- df %>%
group_by(month) %>%
summarize(month_total = n())
word_month_counts <- review_words %>%
filter(word_total >= 1000) %>%
count(word, month) %>%
complete(word, month, fill = list(n = 0)) %>%
inner_join(reviews_per_month, by = « month ») %>%
mutate(percent = n / month_total) %>%
mutate(year = year(month) + yday(month) / 365)
mod <- ~ glm(cbind(n, month_total - n) year, ., family =binomial>%
nest(-word) %>%
mutate(model = map(data, mod)) %>%
unnest(map(model, tidy)) %>%
filter(term == « year ») %>%
arrange(desc(estimate))
slopes %>%
head(9) %>%
inner_join(word_month_counts, by = « word ») %>%
mutate(word = reorder(word, -estimate)) %>%
ggplot(aes(month, n / month_total, color = word)) +
geom_line(show.legend = FALSE) +
scale_y_continuous(labels = percent_format()) +
facet_wrap(~ word, scales = « free_y ») +
expand_limits(y = 0) +
labs(x = « Year »,
y = « Percentage of reviews containing this word »,
title = « 9 fastest growing words in TripAdvisor reviews »,
subtitle = « Judged by growth rate over 15 years »)

You can see a peak of discussion around "Friday Fireworks" and "Lagoon" before 2010. And it was before 2005 that words like "resort fee" and "busy" grew the fastest.


Conversely, what words are used less and less by customers?

 

slopes %>%
tail(9) %>%
inner_join(word_month_counts, by = « word ») %>%
mutate(word = reorder(word, estimate)) %>%
ggplot(aes(month, n / month_total, color = word)) +
geom_line(show.legend = FALSE) +
scale_y_continuous(labels = percent_format()) +
facet_wrap(~ word, scales = « free_y ») +
expand_limits(y = 0) +
labs(x = « Year »,
y = « Percentage of reviews containing this term »,
title = « 9 fastest shrinking words in TripAdvisor reviews »,
subtitle = « Judged by growth rate over 4 years »)

This shows some topics in which interest has died down since 2010, "breakfast", "upgraded" "prices" and "free".


Let's compare a few chosen words.

 

word_month_counts %>%
filter(word %in% c(« service », « food »)) %>%
ggplot(aes(month, n / month_total, color = word)) +
geom_line(size = 1, alpha = .8) +
scale_y_continuous(labels = percent_format()) +
expand_limits(y = 0) +
labs(x = « Year »,
y = « Percentage of reviews containing this term », title = « service vs food in terms of reviewers interest »)

Service and food seem to have been the two main subjects before 2010 - with a peak of interest in 2003.

 

### Step 7 : Start sentiment analysis

Sentiment analysis applies to texts that express the Customer’s feelings, such as consumer opinions, comments published on social media and platforms (Facebook, Twitter, YouTube, Instagram, etc.) or responses to satisfaction surveys and surveys.
In our case, we aim to determine the attitude of  hotel guest in relation to their past experience or their emotional reaction to the hotel. Attitude can be a judgment or an assessment.
Sentiment analysis identifies the most common positive and negative words in reviews.

 

reviews <- df %>%
filter(!is.na(review_body)) %>%
select(ID, review_body) %>%
group_by(row_number()) %>%
ungroup()
tidy_reviews <- reviews %>%
unnest_tokens(word, review_body)
tidy_reviews <- tidy_reviews %>%
anti_join(stop_words)

bing_word_counts <- tidy_reviews %>%
inner_join(get_sentiments(« bing »)) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()

bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = « free ») +
labs(y = « Contribution to sentiment », x = NULL) +
coord_flip() +
ggtitle(‘Words that contribute to positive and negative sentiment in the reviews’)

With another library.

 

contributions <- tidy_reviews %>%
inner_join(get_sentiments(« afinn »), by = « word ») %>%
group_by(word) %>%
summarize(occurences = n(),
contribution = sum(score))
contributions %>%
top_n(25, abs(contribution)) %>%
mutate(word = reorder(word, contribution)) %>%
ggplot(aes(word, contribution, fill = contribution > 0)) +
ggtitle(‘Words with the greatest contributions to positive/negative
sentiment in reviews’) +
geom_col(show.legend = FALSE) +
coord_flip()


There is a potential problem here, for example, "clean", depending on the context, has a negative feeling if preceded by the word "not". In fact, unigrams will have this negation problem in most cases. This brings us to the next topic.

### Step 8 : Combining bigrams to improve results from sentiment analysis

Bigrams provide context for sentiment analysis. In particular, we want to see how many times words are preceded by a word like "not".

 

AFINN <- get_sentiments("afinn") not_words <- bigrams_separated %>%
filter(word1 == « not ») %>%
inner_join(AFINN, by = c(word2 = « word »)) %>%
count(word2, score, sort = TRUE) %>%
ungroup()

not_words

We see that there are 850 occurrences where the word "a" is preceded by the word "not", and 698 other occurrences where "the" is preceded by "not". However, this information is not significant.

 

bigrams_separated %>%
filter(word1 == « not ») %>%
count(word1, word2, sort = TRUE)

This tells us that in the data, the most common word associated with a sentiment that follows “not” is “worth”, and the second word associated with a common sentiment that follows “not” is “recommend”, which would normally have a score (positive) of 2.


We can then ask ourselves in our data, what words have most contributed to pointing us in the wrong direction?

 

not_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab(« Words preceded by \ »not\ » ») +
ylab(« Sentiment score * number of occurrences ») +
ggtitle(‘The 20 words preceded by « not » that had the greatest contribution to
sentiment scores, positive or negative direction’) +
coord_flip()

The bigrams "not worth", "not great", "not good", "not recommend", "not recommend" and "not like" were the main causes of identification errors, making the text much more positive than it really is.
In addition to "not," there are other words that cancel out the next term. This is the case for "no", "never" and "without". Let's go check them out.

 

negation_words <- c("not", "no", "never", "without") negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = « word »)) %>%
count(word1, word2, score, sort = TRUE) %>%
ungroup()

negated_words %>%
mutate(contribution = n * score,
word2 = reorder(paste(word2, word1, sep = « __ »), contribution)) %>%
group_by(word1) %>%
top_n(12, abs(contribution)) %>%
ggplot(aes(word2, contribution, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ word1, scales = « free ») +
scale_x_discrete(labels = function(x) gsub(« __.+$ », «  », x)) +
xlab(« Words preceded by negation term ») +
ylab(« Sentiment score * # of occurrences ») +
ggtitle(‘The most common positive or negative words to follow negations
such as « no », « not », « never » and « without »‘) +
coord_flip()

It seems that the most important sources of misidentification of a word as positive come from "not worth / great / good / recommend", and the most important source of badly classified negative feeling is "not bad" and "no problem ".

### Step 9 : Identify the most positive and negative opinion

 

sentiment_messages <- tidy_reviews %>%
inner_join(get_sentiments(« afinn »), by = « word ») %>%
group_by(ID) %>%
summarize(sentiment = mean(score),
words = n()) %>%
ungroup() %>%
filter(words >= 5)
sentiment_messages %>%
arrange(desc(sentiment))

What is  most positive opinion left on the hotel?

And the worst comment left on the hotel?
